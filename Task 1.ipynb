{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff4836f",
   "metadata": {},
   "source": [
    "# End-to-end machine learning process for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85a3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fed149",
   "metadata": {},
   "source": [
    "# Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83e35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zucker fabrik</td>\n",
       "      <td>ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lebensmittel kommssionierung</td>\n",
       "      <td>ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geländer biegen</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gebäudeausrüstung technische</td>\n",
       "      <td>ct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kürbiskernöl softgels</td>\n",
       "      <td>ft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text label\n",
       "0                 zucker fabrik    ft\n",
       "1  Lebensmittel kommssionierung    ft\n",
       "2               geländer biegen    mr\n",
       "3  gebäudeausrüstung technische    ct\n",
       "4         kürbiskernöl softgels    ft"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and display the dataset\n",
    "file_path = 'sample_data_for_task1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4807a3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                           text label\n",
       " 0                 zucker fabrik    ft\n",
       " 1  lebensmittel kommssionierung    ft\n",
       " 2               geländer biegen    mr\n",
       " 3  gebäudeausrüstung technische    ct\n",
       " 4         kürbiskernöl softgels    ft,\n",
       " label\n",
       " ft     11226\n",
       " pkg     9617\n",
       " ct      5061\n",
       " mr      5016\n",
       " ch      3688\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preparation:\n",
    "\n",
    "# Removing rows with missing labels\n",
    "data_clean = data.dropna(subset=['label'])\n",
    "\n",
    "# Lowercasing the text\n",
    "data_clean['text'] = data_clean['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Analyze label distribution\n",
    "label_distribution = data_clean['label'].value_counts()\n",
    "\n",
    "# Display the cleaned data and label distribution\n",
    "data_clean.head(), label_distribution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1ac3d",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e42a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data_clean['text'])\n",
    "y = data_clean['label']\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0779d",
   "metadata": {},
   "source": [
    "- I used a TF-IDF vectorizer, which is a standard and effective method for converting text data into numerical features. This approach helps the model to understand the importance of words relative to their frequency across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800d23b",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a24d8",
   "metadata": {},
   "source": [
    "For this task I am deploying 3 different ML models:\n",
    "- Logistic regression\n",
    "- SVC\n",
    "- Random forest\n",
    "\n",
    "Later on I will choose the best model among them.\n",
    "For each model, hpyerparameter tuning and cross validation based on accuracy has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06b053",
   "metadata": {},
   "source": [
    "### Logistics regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa540a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'logistic__C': 10.0}\n",
      "Best Score: 0.8819734146063822\n",
      "Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ch       0.88      0.89      0.88       706\n",
      "         cnc       0.88      0.73      0.80       513\n",
      "          ct       0.95      0.88      0.91      1022\n",
      "          ft       0.87      0.93      0.90      2281\n",
      "          mr       0.89      0.82      0.86      1009\n",
      "         pkg       0.87      0.90      0.88      1908\n",
      "\n",
      "    accuracy                           0.88      7439\n",
      "   macro avg       0.89      0.86      0.87      7439\n",
      "weighted avg       0.88      0.88      0.88      7439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline with LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('logistic', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'logistic__C': [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_model_LR = grid_search.best_estimator_\n",
    "y_pred = best_model_LR.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluation_report = classification_report(y_test, y_pred)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Evaluation Report:\")\n",
    "print(evaluation_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea0c60",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "- Logistic Regression is a solid choice for a baseline model in text classification tasks due to its simplicity, interpretability, and efficiency in training.\n",
    "- I have implemented hyperparameter tuning for the Logistic Regression model using GridSearchCV. This is crucial as it systematically searches through a range of parameters to find the best performing model configuration. \n",
    "- Cross-validation (CV=5) during hyperparameter tuning was employed to enhances the model's reliability by ensuring that its performance is consistent across different subsets of the training data.\n",
    "- Evaluate the Logistic Regression model using classification report, which provides a detailed insight into the model's performance across all classes. Metrics like precision, recall, and F1-score are crucial for understanding the model's strengths and weaknesses in classifying each category.\n",
    "- The best 'C' parameter for Logistic Regression is 10.0. This indicates that a lower regularization strength helped the model perform better.\n",
    "- The cross-validation score of approximately 0.882 is quite strong, suggesting that the model generalizes well across different subsets of the data.\n",
    "- The model has high precision across all categories, particularly for 'ct' at 0.95, indicating a low rate of false positives.\n",
    "- Recall values are also strong, though 'cnc' at 0.73 indicates some room for improvement in capturing all relevant instances for this category.\n",
    "- The F1-scores, which balance precision and recall, are consistently high, demonstrating the model's robustness. The 'ct' category shows particularly strong performance with an F1-score of 0.90.\n",
    "- The overall accuracy of 0.88 is very good.\n",
    "\n",
    "The model is performing well, especially in distinguishing between most categories. The high precision and recall for most classes indicate that the model is effective in classifying the text phrases accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73577388",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f50f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10.0, 'kernel': 'rbf'}\n",
      "Best Score: 0.8783103058941932\n",
      "Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ch       0.88      0.88      0.88       706\n",
      "         cnc       0.90      0.72      0.80       513\n",
      "          ct       0.96      0.86      0.90      1022\n",
      "          ft       0.85      0.94      0.89      2281\n",
      "          mr       0.90      0.81      0.86      1009\n",
      "         pkg       0.88      0.91      0.89      1908\n",
      "\n",
      "    accuracy                           0.88      7439\n",
      "   macro avg       0.90      0.85      0.87      7439\n",
      "weighted avg       0.89      0.88      0.88      7439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_model_SVC = grid_search.best_estimator_\n",
    "y_pred = best_model_SVC.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluation_report = classification_report(y_test, y_pred)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Evaluation Report:\")\n",
    "print(evaluation_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451eae76",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "- The best parameters found through hyperparameter tuning are a 'C' value of 10.0 and the 'rbf' kernel.\n",
    "- The cross-validation score of approximately 0.878 is slightly lower than that of the Logistic Regression model but still indicates strong generalization capability.\n",
    "- The precision is high across all classes, notably for 'ct' and 'mr' with 0.96 and 0.90, respectively. This indicates a low rate of false positives for these classes.\n",
    "- The recall shows some variability, with 'cnc' at 0.72, suggesting some difficulty in capturing all relevant instances for this class. However, 'ft' and 'pkg' show strong recall, indicating effective identification of positive instances.\n",
    "- The F1-scores are consistently high, with 'ft' showing a notable score of 0.89, indicating a balanced performance between precision and recall.\n",
    "- With an overall accuracy of 0.88, the SVM model performs comparably to the Logistic Regression model, indicating effective classification capability.\n",
    "\n",
    "The SVM model demonstrates strong classification performance, especially in terms of precision.\n",
    "The comparison between the SVM and Logistic Regression models shows that they have similar overall performance metrics, but differences in precision and recall for specific classes may guide the choice of one model over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ab103",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d0b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Score: 0.8767979380893787\n",
      "Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ch       0.82      0.89      0.85       706\n",
      "         cnc       0.90      0.78      0.84       513\n",
      "          ct       0.95      0.86      0.90      1022\n",
      "          ft       0.88      0.93      0.91      2281\n",
      "          mr       0.89      0.81      0.85      1009\n",
      "         pkg       0.89      0.91      0.90      1908\n",
      "\n",
      "    accuracy                           0.89      7439\n",
      "   macro avg       0.89      0.87      0.87      7439\n",
      "weighted avg       0.89      0.89      0.89      7439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_model_RF = grid_search.best_estimator_\n",
    "y_pred = best_model_RF.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluation_report = classification_report(y_test, y_pred)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Evaluation Report:\")\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70d3f6",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "- The optimal parameters found through tuning include 'max_depth' as None, 'min_samples_leaf' as 1, 'min_samples_split' as 2, and 'n_estimators' as 200.\n",
    "- The cross-validation score of approximately 0.877 is competitive, indicating that the Random Forest model has good generalization capabilities, though it's slightly lower than the Logistic Regression and SVM models.\n",
    "- The model shows strong precision across most classes, particularly for 'cnc', 'ct', and 'mr', indicating a low rate of false positives.\n",
    "- The recall values are generally high, with 'ch' showing an impressive recall of 0.89. The model appears to be effective in identifying relevant instances across classes.\n",
    "- The F1-scores are consistent, reflecting a balanced performance between precision and recall. The 'ft' category stands out with an F1-score of 0.91.\n",
    "- An overall accuracy of 0.89 is notable, suggesting that the Random Forest model is very effective in classifying the text data.\n",
    "\n",
    "The Random Forest model demonstrates robust performance, with particular strength in recall, suggesting it is quite reliable in identifying positive instances across different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178421d",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa7a93",
   "metadata": {},
   "source": [
    "- While Logistic Regression offers straightforward interpretation due to its linear decision boundaries, it falls short in capturing the non-linear relationships present in the data, rendering it more susceptible to overfitting. Therefore, opting for models like SVM with an RBF kernel and Random Forest proves wiser.\n",
    "- Random Forest is inherently adept at handling complex and non-linear relationships in data due to its ensemble approach, combining multiple decision trees to produce a more robust and generalized model.\n",
    "- With Random Forest, I have the flexibility to increase the complexity of the model (e.g., number of trees, depth of trees) without the immediate risk of overfitting.\n",
    "- In my evaluations, Random Forest slightly outperformed the other models in terms of accuracy and maintained high precision and recall across different classes. \n",
    "- Random Forest offers insights into feature importance, which can be beneficial for understanding the driving factors behind the model's decisions.\n",
    "\n",
    "In conclusion, Random Forest is the recommended model to proceed with for deployment, offering a strong balance of performance, complexity handling and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68220c2b",
   "metadata": {},
   "source": [
    "## REST API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49f1baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best trained Random Forest model\n",
    "joblib.dump(best_model_RF, 'random_forest_model.joblib')\n",
    "\n",
    "# Save the TF-IDF vectorizer to transform new text data for predictions\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c267f",
   "metadata": {},
   "source": [
    "### Setting up a REST API\n",
    "\n",
    "- The script starts by loading the trained Random Forest model and the TF-IDF vectorizer using joblib. \n",
    "- I have defined a Pydantic model (TextData) to specify the expected input data format. \n",
    "- The /predict/ endpoint is designed to receive POST requests with text data and return the model's prediction. The endpoint uses the trained vectorizer to transform the incoming text into the same format the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffc964",
   "metadata": {},
   "source": [
    "# Future Work Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1e1d9",
   "metadata": {},
   "source": [
    "- Model Exploration and Ensemble Techniques: Experiment with more advanced models like deep learning architectures (e.g., LSTM, GRU) that might capture the nuances of language better for text classification tasks.\n",
    "\n",
    "- Investigate additional feature engineering techniques beyond TF-IDF, such as word embeddings (Word2Vec, GloVe) or doc2vec, to capture semantic meanings of words.\n",
    "\n",
    "- Implement data augmentation strategies for text data, such as synonym replacement, back-translation, or text generation, to increase the diversity of the training dataset, which can be particularly beneficial for underrepresented classes.\n",
    "\n",
    "- Create a feedback loop where new data or corrected labels can be used to retrain and continuously improve the model.\n",
    "\n",
    "- Creating visualization for better understanding the data and perormance of various models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bfbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
